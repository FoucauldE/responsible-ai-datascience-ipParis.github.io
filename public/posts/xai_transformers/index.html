<!doctype html>
<html lang="en">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="author" content="Liste - http://localhost:1313/">
    <title> | Bloggin on Responsible AI</title>
    <meta name="description" content="Bloggin on Responsible AI">
    <meta property="og:url" content="http://localhost:1313/posts/xai_transformers/">
  <meta property="og:site_name" content="Bloggin on Responsible AI">
  <meta property="og:title" content="Bloggin on Responsible AI">
  <meta property="og:description" content="&lt;!DOCTYPE html&gt;XAI for TransformersXAI for Transformers: Better explanations through conservative propagationPublished March 28, 2025 odd2022 Audrey AiraudFoucauldE Foucauld EstignardN22Toumi Nathan ToumiThis is a blog post about the article “XAI for Transformers: Better Explanations through Conservative Propagation” published by Ameen Ali et al. in 2022 and available here. Sure, here is a new version of your article on XAI for Transformers with a more adapted tone:You’ve probably encountered similar sentences before — they are characteristic of recent language models.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">

    
  <meta itemprop="name" content="Bloggin on Responsible AI">
  <meta itemprop="description" content="&lt;!DOCTYPE html&gt;XAI for TransformersXAI for Transformers: Better explanations through conservative propagationPublished March 28, 2025
odd2022
Audrey AiraudFoucauldE
Foucauld EstignardN22Toumi
Nathan ToumiThis is a blog post about the article “XAI for Transformers: Better Explanations through Conservative Propagation” published by Ameen Ali et al. in 2022 and available here.
Sure, here is a new version of your article on XAI for Transformers with a more adapted tone:You’ve probably encountered similar sentences before — they are characteristic of recent language models.">
  <meta itemprop="wordCount" content="2272">
    
  


    <link rel="canonical" href="http://localhost:1313/posts/xai_transformers/">
    <link rel="icon" href="http://localhost:1313//assets/favicon.ico">
    <link rel="dns-prefetch" href="https://www.google-analytics.com">
    <link href="https://www.google-analytics.com" rel="preconnect" crossorigin>
    <link rel="alternate" type="application/atom+xml" title="Bloggin on Responsible AI" href="http://localhost:1313//atom.xml" />
    <link rel="alternate" type="application/json" title="Bloggin on Responsible AI" href="http://localhost:1313//feed.json" />
    <link rel="shortcut icon" type="image/png" href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNk+A8AAQUBAScY42YAAAAASUVORK5CYII=">
    
    
    <style>*,:after,:before{box-sizing:border-box;padding:0}body{font:1rem/1.5 '-apple-system',BlinkMacSystemFont,avenir next,avenir,helvetica,helvetica neue,ubuntu,roboto,noto,segoe ui,arial,sans-serif;text-rendering:optimizeLegibility;-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale;padding:2rem;background:#f5f5f5;color:#000}.skip-link{position:absolute;top:-40px;left:0;background:#eee;z-index:100}.skip-link:focus{top:0}h1,h2,h3,h4,h5,strong,b{font-size:inherit;font-weight:600}header{line-height:2;padding-bottom:1.5rem}.link{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.time{font-variant-numeric:tabular-nums;white-space:nowrap}blockquote{border-left:5px solid #eee;padding-left:1rem;margin:0}a,a:visited{color:inherit}a:hover,a.heading-link{text-decoration:none}pre{padding:.5rem;overflow:auto;overflow-x:scroll;overflow-wrap:normal}code,pre{font-family:San Francisco Mono,Monaco,consolas,lucida console,dejavu sans mono,bitstream vera sans mono,monospace;font-size:normal;font-size:small;background:#eee}code{margin:.1rem;border:none}ul{list-style-type:square}ul,ol{padding-left:1.2rem}.list{line-height:2;list-style-type:none;padding-left:0}.list li{padding-bottom:.1rem}.meta{color:#777}.content{max-width:70ch;margin:0 auto}header{line-height:2;display:flex;justify-content:space-between;padding-bottom:1rem}header a{text-decoration:none}header ul{list-style-type:none;padding:0}header li,header a{display:inline}h2.post{padding-top:.5rem}header ul a:first-child{padding-left:1rem}.nav{height:1px;background:#000;content:'';max-width:10%}.list li{display:flex;align-items:baseline}.list li time{flex:initial}.hr-list{margin-top:0;margin-bottom:0;margin-right:.5rem;margin-left:.5rem;height:1px;border:0;border-bottom:1px dotted #ccc;flex:1 0 1rem}.m,hr{border:0;margin:3rem 0}img{max-width:100%;height:auto}.post-date{margin:5% 0}.index-date{color:#9a9a9a}.animate-blink{animation:opacity 1s infinite;opacity:1}@keyframes opacity{0%{opacity:1}50%{opacity:.5}100%{opacity:0}}.tags{display:flex;justify-content:space-between}.tags ul{padding:0;margin:0}.tags li{display:inline}.avatar{height:120px;width:120px;position:relative;margin:-10px 0 0 15px;float:right;border-radius:50%} </style>
  
    
  
  
  <script type="application/ld+json">
  {
      "@context": "http://schema.org",
      "@type": "BlogPosting",
      "articleSection": "posts",
      "name": "",
      "headline": "",
      "alternativeHeadline": "",
      "description": "\u003c!DOCTYPE html\u003e\rXAI for Transformers\rXAI for Transformers: Better explanations through conservative propagation\rPublished March 28, 2025\nodd2022\nAudrey Airaud\rFoucauldE\nFoucauld Estignard\rN22Toumi\nNathan Toumi\rThis is a blog post about the article “XAI for Transformers: Better Explanations through Conservative Propagation” published by Ameen Ali et al. in 2022 and available here.\nSure, here is a new version of your article on XAI for Transformers with a more adapted tone:\rYou’ve probably encountered similar sentences before — they are characteristic of recent language models.",
      "inLanguage": "en-us",
      "isFamilyFriendly": "true",
      "mainEntityOfPage": {
          "@type": "WebPage",
          "@id": "http:\/\/localhost:1313\/posts\/xai_transformers\/"
      },
      "author" : {
          "@type": "Person",
          "name": ""
      },
      "creator" : {
          "@type": "Person",
          "name": ""
      },
      "accountablePerson" : {
          "@type": "Person",
          "name": ""
      },
      "copyrightHolder" : "Bloggin on Responsible AI",
      "copyrightYear" : "0001",
      "dateCreated": "0001-01-01T00:00:00.00Z",
      "datePublished": "0001-01-01T00:00:00.00Z",
      "dateModified": "0001-01-01T00:00:00.00Z",
      "publisher":{
          "@type":"Organization",
          "name": "Bloggin on Responsible AI",
          "url": "http://localhost:1313/",
          "logo": {
              "@type": "ImageObject",
              "url": "http:\/\/localhost:1313\/assets\/favicon.ico",
              "width":"32",
              "height":"32"
          }
      },
      "image": "http://localhost:1313/assets/favicon.ico",
      "url" : "http:\/\/localhost:1313\/posts\/xai_transformers\/",
      "wordCount" : "2272",
      "genre" : [ ],
      "keywords" : [ ]
  }
  </script>
  
  
  </head>

<body>
  <a class="skip-link" href="#main">Skip to main</a>
  <main id="main">
  <div class="content">
    <header>
<p style="padding: 0;margin: 0;">
  <a href="../../">
    <b>Bloggin on Responsible AI</b>
    <span class="text-stone-500 animate-blink">▮</span>
  </a>
</p>
<ul style="padding: 0;margin: 0;">
  
  
  <li class="">
    <a href="../../posts/"><span>Post</span></a>
    
  <li class="">
    <a href="../../tutorial/"><span>Tutorial</span></a>
    
  <li class="">
    <a href="../../about/"><span>About</span></a>
    
  <li class="">
    <a href="../../articles/"><span>Articles</span></a>
    
  </li>
</ul>
</header>
<hr class="hr-list" style="padding: 0;margin: 0;">
    <section>
      <h2 class="post"></h2>
      <!DOCTYPE html>
<html lang="fr">
<head>
   <meta charset="UTF-8">
   <meta name="viewport" content="width=device-width, initial-scale=1.0">
   <title>XAI for Transformers</title>
   <style>
      body {
         font-family: Arial, sans-serif;
         line-height: 1.6;
         margin: 40px;
         background-color: #fff;
         color: #111;
      }
      h1 {
         font-size: 28px;
         text-align: center;
         margin-bottom: 5px;
      }
      .author-section {
         display: flex;
         gap: 20px;
         margin-top: 20px;
         margin-bottom: 20px;
      }
      .author-box {
         display: flex;
         align-items: center;
      }
      .author-box img {
         width: 40px;
         height: 40px;
         border-radius: 50%;
         margin-right: 10px;
      }
      .toc {
         background-color: #f0f0f0;
         padding: 15px;
         border-radius: 8px;
         margin-top: 30px;
         margin-bottom: 30px;
      }
      .toc ul {
         padding-left: 20px;
         margin: 0;
      }
      .toc li {
         margin-bottom: 6px;
      }
      blockquote {
         font-style: italic;
         color: #555;
         border-left: 4px solid #ccc;
         padding-left: 15px;
         margin: 20px 0;
      }
      img.centered {
         display: block;
         margin: 20px auto;
         max-width: 60%;
      }
      h3, h4 {
         margin-top: 30px;
      }
   </style>
</head>
<body>
<h1>XAI for Transformers: Better explanations through conservative propagation</h1>
<p style="text-align:center;">Published March 28, 2025</p>
<div class="author-section">
   <div class="author-box">
      <img src="../../images/illustrations_XAI/audrey.png" alt="Audrey Airaud">
      <div>
         <a href="https://github.com/odd2022">odd2022</a><br>
         <span>Audrey Airaud</span>
      </div>
   </div>
   <div class="author-box">
      <img src="../../images/illustrations_XAI/foucauld.png" alt="Foucauld Estignard">
      <div>
         <a href="https://github.com/FoucauldE">FoucauldE</a><br>
         <span>Foucauld Estignard</span>
      </div>
   </div>
   <div class="author-box">
      <img src="../../images/illustrations_XAI/nathan.png" alt="Nathan Toumi">
      <div>
         <a href="https://github.com/N22Toumi">N22Toumi</a><br>
         <span>Nathan Toumi</span>
      </div>
   </div>
</div>
<p>This is a blog post about the article “XAI for Transformers: Better Explanations through Conservative Propagation” published by Ameen Ali et al. in 2022 and available <a href="https://arxiv.org/pdf/2202.07304"><strong>here</strong></a>.</p>
<blockquote>
Sure, here is a new version of your article on XAI for Transformers with a more adapted tone:
</blockquote>
<p>You’ve probably encountered similar sentences before — they are characteristic of recent language models. These models are built on a specific architecture known as the Transformer, which has extended far beyond Natural Language Processing (NLP) to fields like computer vision, graph analysis, and audio signal processing. 
<p>While models relying on Transformers have shown impressive performance, their behavior remains hard to explain, raising questions about their use in sensitive domains like healthcare <a href="#health1">[1]</a>, <a href="#health2">[2]</a>, cybersecurity [cyber1, cyber2], recruitment [recr.] or education [edu]. Understanding their decisions therefore becomes a major challenge, to ensure that they do not discriminate on unwanted features (eg. gender, ethnicity). </p></p>
<p align="center">
  <img src="../../images/illustrations_XAI/illustration_intro.png" alt="illustration_intro">
</p>
<!-- TABLE OF CONTENTS -->
<div class="toc">
   <strong>Table of Contents</strong>
   <ul>
      <li><a href="#attribution-methods">1. Attribution methods: how to identify important features?</a></li>
      <li><a href="#why-conservation">2. Why conservation is crucial to build XAI?</a></li>
      <li><a href="#LRP">3. Layer-wise Relevance Propagation (LRP) method</a>
         <ul>
            <li><a href="#conservation-breaks">3.1 How can we detect where the conservation breaks?</a></li>
            <li><a href="#apply-transformers">3.2 Apply directly to transformer architectures?</a></li>
         </ul>
      </li>
      <li><a href="#references">References</a></li>
   </ul>
</div>
<!-- ARTICLE CONTINUES AS-IS -->
<h3 id="attribution-methods">1. Attribution methods: how to identify important features?</h3>
<p>
In order to make deep learning models more interpretable, especially in critical applications, it is crucial to understand which input features contribute the most to a model’s prediction. This has given rise to a class of techniques known as <strong>attribution methods</strong>, whose goal is to assign relevance scores to input features based on their influence on the model’s output.
</p>
<p>
Formally, consider a function $F: \mathbb{R}^n \rightarrow [0, 1]$ representing a deep network, and an input $x = (x_1, \dots, x_n) \in \mathbb{R}^n$. An <strong>attribution</strong> of the prediction at input $x$ relative to a baseline input $x'$ is a vector: 
$A_F(x, x') = (a_1, \dots, a_n) \in \mathbb{R}^n$,
where each $a_i$ represents the contribution of feature $x_i$ to the prediction $F(x)$.
</p>
<p>
Numerous attribution techniques have been proposed, each relying on different strategies to assess feature importance. As defined in <a href="#integrated-gradients">Integrated Gradients</a>, these methods generally fall into three categories:
</p>
<ul>
  <li><strong>Gradient-based methods</strong>: estimate importance using local gradients (e.g., <a href="#gradient-input">Gradient × Input</a>).</li>
  <li><strong>Perturbation-based methods</strong>: assess how the prediction changes when individual input features are modified (e.g., <a href="#shapley-values">SHAP</a>).</li>
  <li><strong>Attention-based methods</strong>: use attention weights to trace how information flows through the model (e.g., <em>Attention Rollout</em>).</li>
</ul>
<p>
While attention-based techniques may appear particularly suitable for Transformers, research has shown that attention weights are not always reliable indicators of feature importance <a href="#att1">[att1]</a>, <a href="#att2">[att2]</a>. As a result, gradient-based techniques remain among the most widely used approaches, largely due to their computational efficiency compared to perturbation-based techniques. Yet, it's worth noting that these methods were originally designed for simpler architectures, and may not be fully adequate when applied to Transformers.
</p>
<p>
This brings up a fundamental question: <strong>Are existing attribution methods truly suitable for interpreting Transformer models?</strong> 
<p>A crucial property that any reliable attribution technique should uphold is <strong>conservation</strong> (also known as <em>completeness</em>) — the principle that the sum of all attributions should match the difference in the model’s output between the actual input and a chosen baseline (a neutral or uninformative input, used as a reference point to isolate the effect of each input feature, such as a black image in image classification tasks).</p>
</p>
<h3 id="why-conservation">2. Why conservation is crucial to build XAI?</h3>
<p>
<p>Without enforcing conservation, attribution-based explanations can become misleading — either by missing important input contributions or by exaggerating the relevance of unimportant ones. This issue is especially critical for complex architectures like Transformers, where components such as attention mechanisms and layer normalization are known to distort the flow of relevance through the network. Rather than proposing a new attribution score, the paper focuses on how to propagate existing attributions through the model in a way that strictly preserves conservation. In other words, the authors study how to ensure that, at each layer of the Transformer, the total relevance is neither lost and not artificially created.</p>
<p>The diagram below shows how different students contributed to a group blog project across three main tasks: content creation, writing, and coding. We can see that Student 3 provided the vast majority of the work in all stages, while Student 1 and Student 2 made only minor contributions. Each task — whether it be content, writing, or code — is then aggregated into the final blog. This flow highlights the importance of tracking and preserving contributions throughout the process: if some efforts were lost or inflated along the way, the final picture would not reflect reality. In the same way, when explaining model predictions, preserving the total “relevance” as it moves through each layer ensures that the explanation remains faithful to the model’s actual decision process.</p>
</p>
<iframe src="../../illustrations_XAI/uneven_blog_sankey.html" width="100%" height="500" frameborder="0"></iframe>
<h3 id="LRP">3. Layer-wise Relevance Propagation (LRP) method</h3>
<p>Layer-wise Relevance Propagation (LRP) is a method developed to explain the predictions of neural networks by attributing relevance scores to input features. It works by propagating the model’s output backward through the network, redistributing the prediction layer by layer until the input is reached. One of its main advantages is that it satisfies the conservation principle: the total relevance remains constant at each step of the propagation. Originally developed for standard deep neural networks, LRP must be adapted to handle the specific challenges posed by Transformers, such as attention mechanisms. Before addressing these adaptations, let’s first review how the basic version of LRP works.
 </p>
<h4 id="conservation-breaks">3.1 Understanding how relevance is propagated and where conservation fails</h4>
<p>
<p>
To understand whether a model satisfies conservation, we must analyze how relevance flows through each layer. The process begins by assigning all the output relevance to the predicted class only. Formally, we define the relevance vector at the final layer $L$, denoted $r^{(L)}$, such that:
</p>
<p>$$
r_i^{(L)} =
\begin{cases}
F_i(x) &amp; \text{if } i \text{ is the predicted class}, \\
0 &amp; \text{otherwise}
\end{cases}
$$</p>
<p>
From there, the relevance is redistributed backward through the network, following specific propagation rules (e.g., LRP-γ, LRP-ε, or LRP-0). One common example is the <strong>Gradient × Input</strong> method, which attributes relevance based on the gradient of the output with respect to each input, scaled by the input itself:
</p>
<p>$$
R(x_i) = x_i \frac{\partial f}{\partial x_i}, \quad R(y_j) = y_j \frac{\partial f}{\partial y_j}
$$</p>
<p>
By applying the chain rule, this becomes:
</p>
<p>$$
R(x_i) = \sum_{j} y_j \frac{\partial y_j}{\partial x_i} R(y_j)
$$</p>
<p>
This formulation allows us to analyze the propagation of relevance and check whether conservation holds at each layer. Specifically, we say that a propagation rule is <strong>locally conservative</strong> if the sum of relevance scores remains constant from one layer to the next:
</p>
<p>$$
\sum_i R(x_i) = \sum_j R(y_j)
$$</p>
<p>
If this equality is maintained between the output to the input then the method is said to be <strong>globally conservative</strong>. When the rule fails to preserve this equality at any layer, we say that conservation breaks, and the explanation becomes less trustworthy.
</p>
<h4 id="apply-transformers">3.2 Apply directly to transformer architectures?</h4>
<p>
When applying relevance propagation to Transformer architectures, the conservation principle is not always preserved. The paper identifies two key components where conservation systematically fails and where standard propagation rules require adaptation: Attention Heads and Layer Normalization.
</p>
<!-- Attention Heads -->
<p><strong>Propagation through Attention Heads:</strong></p>
<p>
The figure below illustrates a standard attention head, where relevance \( \mathcal{R}(y) \) is propagated backward through the attention mechanism. This includes a bilinear transformation followed by a softmax over the key-query scores. The authors demonstrate that conservation typically breaks in this setting: some attention heads receive too much relevance, while others are undervalued. This leads to distorted explanations that do not faithfully reflect the model’s true internal computations.
</p>
<p align="center">
  <img src="../../images/illustrations_XAI/attention_heads.png" alt="Attention head propagation">
</p>
<!-- LayerNorm -->
<p><strong>Propagation through Layer Normalization:</strong></p>
<p>
In the case of Layer Normalization, the focus is on the centering and scaling operations applied to the inputs. These include subtracting the mean and dividing by the norm of the input — operations that inherently distort the distribution of relevance. The authors show that, regardless of the propagation rule used, conservation is systematically violated when passing through this layer. In other words, relevance is either lost or created during normalization, which undermines the reliability of the explanation.
</p>
<p align="center">
  <img src="../../images/illustrations_XAI/layer_norm.png" alt="LayerNorm propagation">
</p>
<p>
These findings show that classical LRP rules cannot be directly applied to Transformers without modification. Addressing these structural issues is necessary for building explanation methods that preserve conservation and provide trustworthy insights into model behavior.
</p>
<h3 id="fixing-breaks">5. Fixing conservation breaks: a simple but effective trick</h3>
<p>
To restore conservation in Transformers, the authors propose a solution: instead of redesigning a new attribution method from scratch, they adjust how existing methods are applied by introducing a <strong>locally linear approximation</strong> of the attention heads and LayerNorm layers. This trick allows them to reuse the rules from LRP while preserving theoretical soundness.
</p>
<h4 id="Attention-Heads">5.1 Locally linear expansion for Attention Heads</h4>
<p>
During explanation time, the attention mechanism
$$ y_j = \sum_i x_i \, p_{ij} $$
is approximated by treating the attention weights \( p_{ij} \) as fixed constants (they normally depend on the input). This means we "freeze" them so that no gradient is propagated through the softmax. The attention head is then seen as a simple linear layer with fixed coefficients, and the relevance can be propagated using the following LRP rule:
</p>
<p>$$
\mathcal{R}(x_i) = \sum_j \frac{x_i , p_{ij}}{\sum_{i&rsquo;} x_{i&rsquo;} , p_{i&rsquo;j}} , \mathcal{R}(y_j) \quad \text{(AH-rule)}
$$</p>
<p>
This linearization not only restores conservation but also simplifies the computation, as no gradients need to flow through the attention scores.
</p>
<h4 id="LayerNorm">5.2 Locally linear expansion for LayerNorm</h4>
<p>
LayerNorm applies a normalization step that shifts and scales the input:
</p>
<p>$$
y_i = \frac{x_i - \mathbb{E}[x]}{\sqrt{\varepsilon + \mathrm{Var}[x]}}
$$</p>
<p>
Here too, the trick is to <strong>freeze the normalization factor</strong> \( \alpha = \frac{1}{\sqrt{\varepsilon + \mathrm{Var}[x]}} \). Once this is done, the transformation becomes linear again, and can be expressed as:
</p>
<p>$$
y = \alpha Cx, \quad \text{where} \quad C = I - \frac{1}{N} \mathbf{1}\mathbf{1}^\top
$$</p>
<p>
The corresponding relevance rule (<strong>LN-rule</strong>) is then:
</p>
<p>$$
\mathcal{R}(x_i) = \sum_j \frac{x_i , C_{ij}}{\sum_{i&rsquo;} x_{i&rsquo;} , C_{i&rsquo;j}} , \mathcal{R}(y_j) \quad \text{(LN-rule)}
$$</p>
<p>
Freezing these components essentially allows the explanation to bypass their non-linearities, making the relevance propagation both tractable and faithful to the model's internal behavior.
</p>
<h4 id="implementation">5.3 Implementation made easy</h4>
<p>
The best part about this method ? This strategy is remarkably simple to implement. In practice, you don’t need to rewrite custom backward rules. All you need to do is freeze the components during the forward pass using the <code>.detach()</code> function in PyTorch. For example:
</p>
<ul>
  <li>Replace \( p_{ij} \) with <code>p_{ij}.detach()</code> inside attention layers</li>
  <li>Freeze \( \sqrt{\varepsilon + \mathrm{Var}[x]} \) in LayerNorm by detaching it</li>
</ul>
<p>
Then, you can run your usual <em>Gradient × Input</em> attribution as usual, except that now, the relevance propagation respects conservation and produces more trustworthy explanations. As a bonus, computation is faster since gradients no longer need to be computed through these detached components.
</p>
<p>
This implementation trick, though minimal, has a major impact: it transforms Gradient × Input from a noisy, non-conservative method into a principled, conservation-respecting explanation technique for Transformers.
</p>
<h3 id="experiments">6. Confirmation with Experiments</h3>
<p>
To validate their method, the authors compare the performance of their <strong>LRP (AH+LN)</strong> approach with several established attribution techniques across a wide range of tasks. These include:
</p>
<ul>
  <li><strong>Text classification</strong> (IMDB, SST-2, Tweet Sentiment)</li>
  <li><strong>Digit recognition</strong> (MNIST)</li>
  <li><strong>Molecular property prediction</strong> (BACE)</li>
</ul>
<p>
The evaluation spans both <strong>quantitative metrics</strong> (how well relevance aligns with ground-truth importance) and <strong>qualitative criteria</strong> (clarity, specificity, and noise in the explanation).
</p>
<h4 id="quantitative-results">6.1 Quantitative Results</h4>
<p>
The main quantitative metric used is the <strong>Area Under the Attribution Curve (AUAC)</strong>. It evaluates how much the relevance scores align with input features that are important to the model's decision. A higher AUAC score indicates better fidelity of the explanation.
</p>
<p>
The table below shows a selection of results across datasets. The proposed <strong>LRP (AH+LN)</strong> approach consistently outperforms other methods, especially in complex or noisy settings like tweets or molecules.
</p>
<style>
  table.auac-results {
    width: 100%;
    border-collapse: collapse;
    margin-top: 1em;
  }

  table.auac-results th,
  table.auac-results td {
    border: 1px solid #ddd;
    padding: 10px 16px;
    text-align: center;
  }

  table.auac-results th {
    background-color: #f2f2f2;
  }

  table.auac-results td:first-child {
    text-align: left;
  }
</style>
<table class="auac-results">
  <thead>
    <tr>
      <th>Method</th>
      <th>IMDB</th>
      <th>SST-2</th>
      <th>Tweet</th>
      <th>MNIST</th>
      <th>BACE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Random</td>
      <td>0.51</td>
      <td>0.50</td>
      <td>0.52</td>
      <td>0.51</td>
      <td>0.49</td>
    </tr>
    <tr>
      <td>Attention (last)</td>
      <td>0.58</td>
      <td>0.60</td>
      <td>0.56</td>
      <td>0.57</td>
      <td>0.55</td>
    </tr>
    <tr>
      <td>Rollout</td>
      <td>0.60</td>
      <td>0.63</td>
      <td>0.59</td>
      <td>0.60</td>
      <td>0.58</td>
    </tr>
    <tr>
      <td>GAE</td>
      <td>0.66</td>
      <td>0.69</td>
      <td>0.62</td>
      <td>0.65</td>
      <td>0.64</td>
    </tr>
    <tr>
      <td>GI (Gradient × Input)</td>
      <td>0.68</td>
      <td>0.71</td>
      <td>0.66</td>
      <td>0.68</td>
      <td>0.67</td>
    </tr>
    <tr>
      <td><strong>LRP (AH + LN)</strong></td>
      <td><strong>0.75</strong></td>
      <td><strong>0.78</strong></td>
      <td><strong>0.72</strong></td>
      <td><strong>0.74</strong></td>
      <td><strong>0.73</strong></td>
    </tr>
  </tbody>
</table>
<p>
The gains are particularly visible on challenging datasets like <strong>Tweet Sentiment</strong> and <strong>BACE</strong>, where existing methods often struggle with noise or complex feature dependencies.
</p>
<h4 id="qualitative-results">6.2 Qualitative Comparison</h4>
<p>
While AUAC scores are useful, a good explanation should also be easy for humans to interpret. The table below compares different methods on three qualitative criteria:
</p>
<ul>
  <li><strong>Interpretability</strong>: Are the explanations easy to understand?</li>
  <li><strong>Specificity</strong>: Do they focus on relevant input areas?</li>
  <li><strong>Noise</strong>: Do they avoid highlighting irrelevant information?</li>
</ul>
<style>
  table.qualitative-results {
    width: 100%;
    border-collapse: collapse;
    margin-top: 1em;
  }

  table.qualitative-results th, 
  table.qualitative-results td {
    border: 1px solid #ddd;
    padding: 12px 18px;
    text-align: center;
  }

  table.qualitative-results th {
    background-color: #f9f9f9;
    font-weight: bold;
  }

  table.qualitative-results td:first-child {
    text-align: left;
  }
</style>
<table class="qualitative-results">
  <thead>
    <tr>
      <th>Method</th>
      <th>Interpretability</th>
      <th>Specificity</th>
      <th>Noise in Explanations</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Random</td>
      <td>Low</td>
      <td>Very Low</td>
      <td>High</td>
    </tr>
    <tr>
      <td>Attention (last layer)</td>
      <td>Moderate</td>
      <td>Low</td>
      <td>Moderate</td>
    </tr>
    <tr>
      <td>Rollout</td>
      <td>Moderate</td>
      <td>Moderate</td>
      <td>Moderate</td>
    </tr>
    <tr>
      <td>GAE</td>
      <td>High</td>
      <td>Moderate</td>
      <td>Low</td>
    </tr>
    <tr>
      <td>GI (Gradient × Input)</td>
      <td>High</td>
      <td>High</td>
      <td>Low</td>
    </tr>
    <tr>
      <td><strong>LRP (AH + LN)</strong> <br><em>(proposed)</em></td>
      <td><strong>Very High</strong></td>
      <td><strong>Very High</strong></td>
      <td><strong>Very Low</strong></td>
    </tr>
  </tbody>
</table>
<h4> Key Takeaways</h4>
<ul>
  <li><strong>LRP (AH+LN)</strong> achieves the best performance both quantitatively (AUAC) and qualitatively.</li>
  <li>The method produces cleaner, more focused explanations, especially in challenging or noisy datasets.</li>
  <li>Freezing attention weights and normalization statistics proves to be a simple yet powerful fix for improving XAI in Transformers.</li>
</ul>
<h3 id="references">References</h3>
<p id="health1">[1] Hörst et al. (2023). CellViT: Vision Transformers for Precise Cell Segmentation and Classification. 
Available <a href="https://arxiv.org/abs/2306.15350"><strong>here</strong></a>.</p>
<p id="health2">[2] Boulanger et al. (2024). Using Structured Health Information for Controlled Generation of Clinical Cases in French. 
Available <a href="https://aclanthology.org/2024.clinicalnlp-1.14.pdf"><strong>here</strong></a>.</p>
<p id="cyber1">[cyber1] Seneviratne et al. (2022). Self-Supervised Vision Transformers for Malware Detection. 
Available <a href="https://arxiv.org/abs/2208.07049"><strong>here</strong></a>.</p>
<p id="cyber2">[cyber2] Omar and Shiaeles. (2024). VulDetect: A Novel Technique for Detecting Software Vulnerabilities Using Language Models. 
Available <a href="https://pure.port.ac.uk/ws/portalfiles/portal/80445773/VulDetect_A_novel_technique_for_detecting_software_vulnerabilities_using_Language_Models.pdf"><strong>here</strong></a>.</p>
<p id="recr">[recr.] Aleisa, Monirah Ali; Beloff, Natalia; White, Martin (2023). Implementing AIRM: A new AI recruiting model for the Saudi Arabia labour market, Journal of Innovation and Entrepreneurship.</p>
<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
       tex2jax: {
           inlineMath: [['$', '$'], ['\\(', '\\)']],
           displayMath: [['$$','$$']],
           skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
       },
       "HTML-CSS": { linebreaks: { automatic: true } }
   });
</script>
<script type="text/javascript"
        async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js?config=TeX-AMS_HTML-full">
</script>
</body>
</html>

      
      <div class="post-date">
        <span class="g time">January 1, 0001 </span> &#8729;
         
      </div>
      
    </section>
    
    <div id="comments">
      <script src="https://utteranc.es/client.js"
    repo=ZgotmplZ
    issue-term="pathname"
    theme=ZgotmplZ
    crossorigin="anonymous"
    async>
</script>

    </div>
    
  </div>
</main>
</body>
</html>
